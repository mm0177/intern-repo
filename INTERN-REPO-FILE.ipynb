{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su7_hxN6HRPD"
      },
      "source": [
        "**DEPENDENCIES** **INSTALLATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EGA3o62LD17",
        "outputId": "0ccc8893-8923-4270-9196-595b85c9cc20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.7/306.7 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\n",
            "added 22 packages in 2s\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0KCollecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit sentence-transformers qdrant-client transformers pdfplumber rank_bm25 nest-asyncio pyngrok\n",
        "!npm install -g localtunnel\n",
        "!pip install PyPDF2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_E0o-H6HcCP"
      },
      "source": [
        "**CODE** **HERE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaV6_UJ-LIDS",
        "outputId": "8a53065a-9965-4f0c-c0cc-08ba777ef78e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "App URL: https://d966-34-125-198-225.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import nest_asyncio\n",
        "import subprocess\n",
        "from pyngrok import ngrok\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import PyPDF2\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "streamlit_script = \"\"\"\\\n",
        "import streamlit as st\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client import QdrantClient, models\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import PyPDF2\n",
        "import io\n",
        "import torch\n",
        "import asyncio\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "from typing import List\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "\n",
        "@st.cache_resource\n",
        "def load_models():\n",
        "    try:\n",
        "        embedder = SentenceTransformer('BAAI/bge-m3')\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", token=\"INCLUDE YOUR HF TOKEN HERE \")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16,\n",
        "            attn_implementation=\"eager\",\n",
        "            token=\"INCLUDE YOUR HF TOKEN HERE \"\n",
        "        )\n",
        "        return embedder, tokenizer, model\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading models: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "def process_document(file) -> List[str]:\n",
        "    try:\n",
        "        if file.name.endswith('.pdf'):\n",
        "            pdf_reader = PyPDF2.PdfReader(io.BytesIO(file.read()))\n",
        "            text = \"\"\n",
        "            for page in pdf_reader.pages:\n",
        "                text += page.extract_text() or \"\"\n",
        "        else:\n",
        "            text = file.read().decode('utf-8')\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=700,\n",
        "            chunk_overlap=200,\n",
        "            separators=[\"\\\\n\\\\n\", \"\\\\n\", \". \", \"! \", \"? \", \"。\", \"．\"]\n",
        "        )\n",
        "        chunks = splitter.split_text(text)\n",
        "        return chunks if chunks else []\n",
        "    except Exception as e:\n",
        "        st.error(f\"Document Error: {e}\")\n",
        "        return []\n",
        "\n",
        "@st.cache_resource\n",
        "def init_qdrant():\n",
        "    try:\n",
        "        client = QdrantClient(\":memory:\")\n",
        "        client.recreate_collection(\n",
        "            collection_name=\"doc_chunks\",\n",
        "            vectors_config=models.VectorParams(size=1024, distance=models.Distance.COSINE)\n",
        "        )\n",
        "        return client\n",
        "    except Exception as e:\n",
        "        st.error(f\"Qdrant Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def embed_and_store(chunks: List[str], embedder, client):\n",
        "    embeddings = embedder.encode(chunks, batch_size=32, show_progress_bar=False)\n",
        "    client.upsert(\n",
        "        collection_name=\"doc_chunks\",\n",
        "        points=[models.PointStruct(id=i, vector=e.tolist(), payload={\"text\": c})\n",
        "                for i, (e, c) in enumerate(zip(embeddings, chunks))]\n",
        "    )\n",
        "    return embeddings\n",
        "\n",
        "async def retrieve_chunks(query: str, embedder, client, chunks: List[str], embeddings: np.ndarray, k=5) -> List[str]:\n",
        "    query_embedding = embedder.encode([query])[0]\n",
        "    dense_results = client.search(\n",
        "        collection_name=\"doc_chunks\",\n",
        "        query_vector=query_embedding.tolist(),\n",
        "        limit=k*3\n",
        "    )\n",
        "    dense_chunks = [p.payload[\"text\"] for p in dense_results]\n",
        "    tokenized_chunks = [c.split() for c in chunks]\n",
        "    bm25 = BM25Okapi(tokenized_chunks)\n",
        "    sparse_scores = bm25.get_scores(query.split())\n",
        "    sparse_chunks = [chunks[i] for i in np.argsort(sparse_scores)[-k*3:][::-1]]\n",
        "    combined = list(set(dense_chunks + sparse_chunks))\n",
        "    unique_chunks = []\n",
        "    for chunk in combined:\n",
        "        if not unique_chunks:\n",
        "            unique_chunks.append(chunk)\n",
        "            continue\n",
        "        chunk_embed = embedder.encode([chunk])[0]\n",
        "        sims = cosine_similarity([chunk_embed], embedder.encode(unique_chunks))[0]\n",
        "        if max(sims) < 0.85:\n",
        "            unique_chunks.append(chunk)\n",
        "    return unique_chunks[:k]\n",
        "\n",
        "def validate_response(response: str) -> str:\n",
        "    response = re.sub(r'(?i)\\\\b(\\\\w+)\\\\b(?=.*\\\\b\\\\1\\\\b)', '', response)\n",
        "    return \"\\\\n\".join([l.strip() for l in response.split(\"\\\\n\") if l.strip()])\n",
        "\n",
        "def generate_response(query: str, context_chunks: List[str], tokenizer, model) -> str:\n",
        "    context_str = \"\\\\n- \".join(context_chunks)\n",
        "    prompt = (\n",
        "        \"You are a professional document analyst tasked with answering questions based solely on the provided document context. Do not use external knowledge or make assumptions beyond the context. Use the following context:\\\\n\"\n",
        "        f\"{context_str}\\\\n\\\\n\"\n",
        "        f\"Question: {query}\\\\n\"\n",
        "        \"Instructions:\\\\n\"\n",
        "        \"1. Provide a detailed and elaborate explanation based only on the given context\\\\n\"\n",
        "        \"2. Explain each of concept thoroughly\\\\n\"\n",
        "        \"3. Use multiple paragraphs if needed\\\\n\"\n",
        "        \"4. Include specific numbers/dates/percentages when available\\\\n\"\n",
        "        \"5. Format each paragraph as a plain text bullet WITHOUT MARKDOWN:\\\\n\"\n",
        "        \"   - Start each bullet with a dash '-'\\\\n\"\n",
        "        \"   - Put exactly 1 empty line between bullets\\\\n\"\n",
        "        \"   - Never use •, *, or any other markdown symbols\\\\n\"\n",
        "        \"6. Follow this exact pattern:\\\\n\"\n",
        "        \"8. After the bullet points, provide a concise summary paragraph that encapsulates the key points\\\\n\"\n",
        "        \"8. Do not use markdown syntax (e.g., no asterisks or hashes)\\\\n\"\n",
        "        \"9. Avoid repeating the same point multiple times\\\\n\"\n",
        "        \"10. If the context lacks sufficient information, state that clearly and do not speculate\\\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=2000,\n",
        "        temperature=0.3,\n",
        "        top_p=0.7,\n",
        "        repetition_penalty=1.2,\n",
        "        do_sample=True\n",
        "    )\n",
        "    return validate_response(tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"Answer:\")[-1])\n",
        "\n",
        "st.title(\"Document Analysis Chatbot\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload PDF/TXT\", type=[\"pdf\", \"txt\"])\n",
        "if uploaded_file:\n",
        "    with st.spinner(\"Processing...\"):\n",
        "        chunks = process_document(uploaded_file)\n",
        "        if chunks:\n",
        "            embedder, tokenizer, model = load_models()\n",
        "            client = init_qdrant()\n",
        "            if embedder and client:\n",
        "                embeddings = embed_and_store(chunks, embedder, client)\n",
        "                st.session_state.update(chunks=chunks, embeddings=embeddings)\n",
        "                st.success(\"Ready for questions!\")\n",
        "\n",
        "query = st.text_input(\"Ask about the document:\")\n",
        "if query and 'chunks' in st.session_state:\n",
        "    with st.spinner(\"Analyzing...\"):\n",
        "        loop = asyncio.new_event_loop()\n",
        "        asyncio.set_event_loop(loop)\n",
        "        context_chunks = loop.run_until_complete(\n",
        "            retrieve_chunks(query, embedder, client,\n",
        "                           st.session_state['chunks'],\n",
        "                           st.session_state['embeddings'])\n",
        "        )\n",
        "\n",
        "        response = generate_response(query, context_chunks, tokenizer, model)\n",
        "        st.markdown(f\"*Answer:*\\\\n{response}\")\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(streamlit_script)\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"INCLUDE YOUR NGROK AUTH TOKEN HERE\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "subprocess.Popen([\n",
        "    \"streamlit\", \"run\", \"app.py\",\n",
        "    \"--server.port\", \"8501\",\n",
        "    \"--server.enableCORS\", \"false\",\n",
        "\n",
        "    \"--server.enableXsrfProtection\", \"false\"\n",
        "])\n",
        "\n",
        "public_url = ngrok.connect(8501, bind_tls=True)\n",
        "print(f\"App URL: {public_url.public_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdGza-q6LT6s"
      },
      "outputs": [],
      "source": [
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79LU3MuIOy8A"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
